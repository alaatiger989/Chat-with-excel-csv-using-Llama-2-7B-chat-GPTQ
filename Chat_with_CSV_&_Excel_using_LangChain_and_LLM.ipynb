{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0X1EX8eRcRCk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline , GenerationConfig\n",
    "from deep_translator import GoogleTranslator\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odUgyhg_c4Co",
    "outputId": "6f74b7ff-edb6-44da-82a1-8a8b07e1f811"
   },
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/armgilles/194bcff35001e7eb53a2a8b441e8b2c6/raw/92200bc0a673d5ce2110aaad4544ed6c4010f687/pokemon.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TR8dXxxHbY_b"
   },
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "loader = CSVLoader(file_path='ratings.csv' , csv_args={'delimiter': ','})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load_and_split()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: cup\n",
      "Definition of the topic: cup (s)\n",
      "Rate: 0\n",
      "Rating: Zero\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs={\"device\": DEVICE},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.8 s\n",
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    # revision=\"gptq-4bit-128g-actorder_True\",\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=False,\n",
    "    device=DEVICE,\n",
    "    quantize_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "# \"\"\".strip()\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"You are a Rate System can rate any definition from 0 to 10. if your rate is between 0 and 3 respond with '0' , if your rate is between 4 to 7 respond with '1' and if your rate is between 8 to 10 respond with '2'\"\"\".strip()\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a Definition Rate System, you will use the provided context to rate user Definitions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "the provided context, inform the user. Do not use any other information for answering user. Provide the rate only to the question.\"\"\"\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <>\n",
    "{system_prompt}\n",
    "<>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid warning \n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    # top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    do_sample=True,\n",
    "    streamer=streamer,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline)#, model_kwargs={\"temperature\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "# SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "# SYSTEM_PROMPT = \"You are a Definition Rate System . Use the following pieces of context to Give the rate only at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "SYSTEM_PROMPT = \"\"\"You are a Definition Rate System, you will use the provided context to rate user Definitions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "the provided context, inform the user. Do not use any other information for answering user. Provide the rate only to the question.\"\"\"\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
    "from langchain.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    callbacks=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:   Veil is At his death\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Understood! Based on the provided context, I would give the following ratings:\n",
      "\n",
      "Topic: Veil\n",
      "Question: Veil\n",
      "Answer: At his death\n",
      "Rate: 0\n",
      "Veil is At his death\n",
      "\n",
      "> Answer:\n",
      "مفهوم! بناءً على السياق المقدم، سأعطي التقييمات التالية:\n",
      "\n",
      "الموضوع: الحجاب\n",
      "السؤال: الحجاب\n",
      "الجواب: عند وفاته\n",
      "المعدل: 0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:   Veil is A covering that a girl wears on her head when praying and going out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you for using my services! I'll be happy to help you with your questions.\n",
      "Based on the provided context, here are my ratings for each question:\n",
      "1. Topic: Awwal\n",
      "Question: Awwal\n",
      "Answer: 2\n",
      "Grade: 2\n",
      "\n",
      "Explanation: This definition seems to be accurate and relevant to the topic of Awwal. It describes the disappearance or absence of the sun, which is an important concept in many cultures and belief systems.\n",
      "2. Question: Veil is A covering that a girl wears on her head when praying and going out\n",
      "Grade: 1\n",
      "\n",
      "Explanation: This definition is partially accurate but lacks clarity and detail. A veil is indeed a type of garment that some girls wear on their heads, particularly during prayer or social events. However, it would be more helpful if the user provided additional context or details about the purpose or significance of the veil.\n",
      "Veil is A covering that a girl wears on her head when praying and going out\n",
      "\n",
      "> Answer:\n",
      "شكرا لاستخدامك خدماتي! سأكون سعيدًا بمساعدتك في الإجابة على أسئلتك.\n",
      "بناءً على السياق المقدم، إليك تقييماتي لكل سؤال:\n",
      "1. الموضوع: أول\n",
      "السائل : أوال\n",
      "الجواب: 2\n",
      "الصف: 2\n",
      "\n",
      "توضيح: يبدو أن هذا التعريف دقيق ومناسب لموضوع الأول. وهو يصف اختفاء الشمس أو غيابها، وهو مفهوم مهم في العديد من الثقافات والمعتقدات.\n",
      "2. السؤال: الحجاب هو الغطاء الذي تلبسه الفتاة على رأسها عند الصلاة والخروج\n",
      "الصف: 1\n",
      "\n",
      "الشرح: هذا التعريف دقيق جزئيا ولكنه يفتقر إلى الوضوح والتفصيل. الحجاب هو في الواقع نوع من الملابس التي ترتديها بعض الفتيات على رؤوسهن، خاصة أثناء الصلاة أو المناسبات الاجتماعية. ومع ذلك، سيكون من المفيد أكثر أن يقدم المستخدم سياقًا إضافيًا أو تفاصيل حول غرض الحجاب أو أهميته.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:   Veil is scarf that we wear on our heads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm happy to help! Here's my response:\n",
      "Grade: 2\n",
      "\n",
      "Veil is a scarf that we wear on our heads.\n",
      "Veil is scarf that we wear on our heads\n",
      "\n",
      "> Answer:\n",
      "أنا سعيد للمساعدة! وهنا ردي:\n",
      "الصف: 2\n",
      "\n",
      "الحجاب هو وشاح نرتديه على رؤوسنا.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:   Veil is Made of silk and cotton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great! I'm ready to assist you with rating definitions. Based on the context you provided, here's my response:\n",
      "Topic: Veil\n",
      "Question: Is veil made of silk and cotton?\n",
      "Grade: 3\n",
      "Veil is Made of silk and cotton\n",
      "\n",
      "> Answer:\n",
      "عظيم! أنا على استعداد لمساعدتك في تعريفات التصنيف. بناءً على السياق الذي قدمته، إليك ردي:\n",
      "الموضوع: الحجاب\n",
      "السؤال: هل الحجاب مصنوع من الحرير والقطن؟\n",
      "الصف 3\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"\\nEnter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    # Get the answer from the chain\n",
    "    query = GoogleTranslator(source = 'auto' , target = 'en').translate(query)\n",
    "    res = qa_chain(query)\n",
    "    answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "    # Print the result\n",
    "    # print(\"\\n\\n> Question:\")\n",
    "    print(query)\n",
    "    print(\"\\n> Answer:\")\n",
    "    print(GoogleTranslator(source = 'auto' , target = 'ar').translate(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7gSpkQ0eEmM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
